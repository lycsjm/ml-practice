{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 測試 Discriminative Model\n",
    "李宏毅 ML Lecture 5 中，關於 generative model 可能表現比較差的原因有給予一例試著說明\n",
    "但並沒有給予相對應的 discriminative model 測試\n",
    "本 notebook 以此為例實做並驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "        def __init__(self, features, label=None):\n",
    "            self.features = features\n",
    "            self.label = label\n",
    "            \n",
    "        def __str__(self):\n",
    "            return f'[{self.features}, {self.label}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 對 B 做微分\n",
    "$\\begin{align}\n",
    "L(f) & = \\frac{1}{n} \\sum_n -[\\hat y^n(1-\\sigma(z)) - (1- \\hat y^n) \\sigma(z)] \\\\\n",
    "    & = \\frac{1}{n} \\sum_n -[\\hat y^n - \\hat y^n \\sigma(z) - \\sigma(z) + \\hat y^n  \\sigma(z)] \\\\\n",
    "    & = -(\\hat y^n - \\sigma(z))\n",
    "    \\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(data, w, b):\n",
    "    sigmoid = lambda x: 1 / (1 + exp(-x))\n",
    "    return sigmoid(sum(x_i * w_i for x_i, w_i in zip(data.features, w)) + b)\n",
    "\n",
    "def loss(training_data_set, w , b):\n",
    "    error = 0\n",
    "    for d in training_data_set:\n",
    "        error += -(d.label * log(f(d, w, b)) + (1-d.label) * log(1 - f(d, w, b)))\n",
    "    return error\n",
    "\n",
    "def cal_gd_w(training_data_set, w, b):\n",
    "    gd_w = [0] * len(w)\n",
    "    for i, w_i in enumerate(w):\n",
    "        gd_w[i] = sum(-(d.label - f(d, w, b) * d.features[i]) for d in training_data_set) / len(training_data_set)\n",
    "    return gd_w\n",
    "\n",
    "def cal_gd_b(training_data_set, w, b):\n",
    "    gd_b = sum(-(d.label - f(d, w, b)) for d in training_data_set) / len(training_data_set)\n",
    "    return gd_b\n",
    "\n",
    "def initial(sample_distribution):\n",
    "    '''\n",
    "    sample_distribution:\n",
    "        list-like item contain 4 interger, as number of data with feature (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "    '''\n",
    "    training_data_set = []\n",
    "    for sample_number in sample_distribution:\n",
    "        for first_feature in range(2):\n",
    "            for second_feature in range(2):\n",
    "                label = 1 if first_feature and second_feature else 0\n",
    "                training_data_set += [Data((first_feature, second_feature), label) for i in range(sample_number)]\n",
    "\n",
    "    w = [random.random(), random.random()]\n",
    "    b = random.random()\n",
    "    l = 0.3\n",
    "\n",
    "    return training_data_set, w, b, l\n",
    "\n",
    "\n",
    "def print_env(data_set, w, b, l, header='Initial'):\n",
    "    print(f'{header}: w = {w}, b = {b}, learning rate = {l}')\n",
    "    print(f'loss =', loss(data_set, w, b))\n",
    "\n",
    "def print_test(data_set, w, b, header='Initial'):\n",
    "    print(f'{header}: ', end='')\n",
    "    for d in data_set:\n",
    "        print('f({}) = {}'.format(d, f(d, w, b)), end= ', ')\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    training_data_set, w, b, l = initial(sample_distribution=[4, 4, 4, 1])\n",
    "    testing_data_set = [Data((1, 1)), Data((1, 0))]\n",
    "    \n",
    "    print_env(training_data_set, w, b, l)\n",
    "    for times in range(1000):\n",
    "        random.shuffle(training_data_set)\n",
    "        gd_w = cal_gd_w(training_data_set, w, b)\n",
    "        gd_b = cal_gd_b(training_data_set, w, b)\n",
    "        \n",
    "       # Updating weight\n",
    "        b = b - l * gd_b\n",
    "        for i, gd_w in enumerate(gd_w):\n",
    "            w[i] -= l * gd_w\n",
    "            \n",
    "        if not (times+1) % 100:\n",
    "            l *= 0.9\n",
    "            \n",
    "            # Export Env\n",
    "            print_env(training_data_set, w, b, l, 'Round {}'.format(times+1))\n",
    "            print('gd_w = {}, gd_b = {}', gd_w, gd_b)\n",
    "        \n",
    "        if not (times+1) % 50:\n",
    "            print_test(testing_data_set, w, b, header='(Val) Round {}'.format(times+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: w = [0.2202435879124145, 0.8517818131556736], b = 0.6922331722981018, learning rate = 0.3\n",
      "loss = 55.16824499020861\n",
      "(Val) Round 50: f([(1, 1), None]) = 0.49839620688949415, f([(1, 0), None]) = 0.27482459307909063, \n",
      "Round 100: w = [1.3932913709247252, 1.5192876700044209], b = -2.5072551675502437, learning rate = 0.27\n",
      "loss = 15.464820145515748\n",
      "gd_w = {}, gd_b = {} -0.0323360716160868 0.048683321018724225\n",
      "(Val) Round 100: f([(1, 1), None]) = 0.5999661031862927, f([(1, 0), None]) = 0.24713265309449003, \n",
      "(Val) Round 150: f([(1, 1), None]) = 0.6595328832990524, f([(1, 0), None]) = 0.22221568480530335, \n",
      "Round 200: w = [2.2084703628052735, 2.2464779437463394], b = -3.5948428167813185, learning rate = 0.24300000000000002\n",
      "loss = 10.84093140265069\n",
      "gd_w = {}, gd_b = {} -0.02285947713486277 0.03398344343378145\n",
      "(Val) Round 200: f([(1, 1), None]) = 0.7026826937098045, f([(1, 0), None]) = 0.19998750543574254, \n",
      "(Val) Round 250: f([(1, 1), None]) = 0.732847560575323, f([(1, 0), None]) = 0.1826578246739813, \n",
      "Round 300: w = [2.7257505237130397, 2.740898845111393], b = -4.328139844829946, learning rate = 0.21870000000000003\n",
      "loss = 8.586721129931075\n",
      "gd_w = {}, gd_b = {} -0.018241981853365452 0.027016115425430972\n",
      "(Val) Round 300: f([(1, 1), None]) = 0.7574058807247109, f([(1, 0), None]) = 0.1676479391498313, \n",
      "(Val) Round 350: f([(1, 1), None]) = 0.7759639325764626, f([(1, 0), None]) = 0.155858164477409, \n",
      "Round 400: w = [3.09994283686851, 3.1072744061121584], b = -4.870626756690637, learning rate = 0.19683000000000003\n",
      "loss = 7.232818040617937\n",
      "gd_w = {}, gd_b = {} -0.015448684889073834 0.022876601748370036\n",
      "(Val) Round 400: f([(1, 1), None]) = 0.7919286889936714, f([(1, 0), None]) = 0.14545729744912092, \n",
      "(Val) Round 450: f([(1, 1), None]) = 0.8045109348541079, f([(1, 0), None]) = 0.1371156514706337, \n",
      "Round 500: w = [3.3878390451720413, 3.3919203992832663], b = -5.292286927985728, learning rate = 0.17714700000000003\n",
      "loss = 6.329390935320825\n",
      "gd_w = {}, gd_b = {} -0.013575049862227712 0.020118323326112887\n",
      "(Val) Round 500: f([(1, 1), None]) = 0.8156986080337316, f([(1, 0), None]) = 0.12960588953129182, \n",
      "(Val) Round 550: f([(1, 1), None]) = 0.824758248290881, f([(1, 0), None]) = 0.1234680882870789, \n",
      "Round 600: w = [3.6174744105595065, 3.619996191764976], b = -5.630451006493661, learning rate = 0.15943230000000003\n",
      "loss = 5.685979242189558\n",
      "gd_w = {}, gd_b = {} -0.012235375221880227 0.018149758344866488\n",
      "(Val) Round 600: f([(1, 1), None]) = 0.8329971852202913, f([(1, 0), None]) = 0.11784718103929796, \n",
      "(Val) Round 650: f([(1, 1), None]) = 0.8397984520736436, f([(1, 0), None]) = 0.11318166471812412, \n",
      "Round 700: w = [3.8050875550463723, 3.806775468970552], b = -5.907633225297139, learning rate = 0.14348907000000002\n",
      "loss = 5.206719157934146\n",
      "gd_w = {}, gd_b = {} -0.011234229297134662 0.01667868949757062\n",
      "(Val) Round 700: f([(1, 1), None]) = 0.846086363886221, f([(1, 0), None]) = 0.10884964163155851, \n",
      "(Val) Round 750: f([(1, 1), None]) = 0.8513523888498672, f([(1, 0), None]) = 0.10520890152569069, \n",
      "Round 800: w = [3.9610360799187476, 3.9622391031463415], b = -6.138522696360218, learning rate = 0.12914016300000003\n",
      "loss = 4.837779262308253\n",
      "gd_w = {}, gd_b = {} -0.010461382793637647 0.015542384499414105\n",
      "(Val) Round 800: f([(1, 1), None]) = 0.8562827102900188, f([(1, 0), None]) = 0.10179049463668548, \n",
      "(Val) Round 850: f([(1, 1), None]) = 0.8604582974320142, f([(1, 0), None]) = 0.09888845620430228, \n",
      "Round 900: w = [4.092365378727477, 4.093267037338996], b = -6.33324844305191, learning rate = 0.11622614670000003\n",
      "loss = 4.54651756684651\n",
      "gd_w = {}, gd_b = {} -0.009849775631495278 0.014642426649971257\n",
      "(Val) Round 900: f([(1, 1), None]) = 0.864406765827, f([(1, 0), None]) = 0.09613877948084935, \n",
      "(Val) Round 950: f([(1, 1), None]) = 0.86778083083122, f([(1, 0), None]) = 0.09378510373434096, \n",
      "Round 1000: w = [4.204095064324466, 4.204799085684789], b = -6.49909073472802, learning rate = 0.10460353203000003\n",
      "loss = 4.3119679521531245\n",
      "gd_w = {}, gd_b = {} -0.00935621402810459 0.013915579866148591\n",
      "(Val) Round 1000: f([(1, 1), None]) = 0.8709970609735662, f([(1, 0), None]) = 0.09153826633765161, \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
