{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 測試 Discriminative Model\n",
    "李宏毅 ML Lecture 5 中，關於 generative model 可能表現比較差的原因有給予一例試著說明\n",
    "但並沒有給予相對應的 discriminative model 測試\n",
    "本 notebook 以此為例實做並驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model 的相關函數\n",
    "\n",
    "* Step1 function set: `f()` is a  logistic model\n",
    "* Step 2 goodness of function: `loss() `\n",
    "* Step 3 find the best function:\n",
    "    * `cal_gradient()` to find gradient\n",
    "    * `gradient_descient()` to update\n",
    "    * 分兩個函數是因為想看看 gradient\n",
    "\n",
    "### Gradient of Loss Function\n",
    "#### 對 W 微分\n",
    "$L(f) = -(\\hat y^n - f_{w, b}(x^n))x_i^n$\n",
    "#### 對 B 微分\n",
    "$\\begin{align}\n",
    "L(f) & = \\frac{1}{n} \\sum_n -[\\hat y^n(1-\\sigma(z)) - (1- \\hat y^n) \\sigma(z)] \\\\\n",
    "    & = \\frac{1}{n} \\sum_n -[\\hat y^n - \\hat y^n \\sigma(z) - \\sigma(z) + \\hat y^n  \\sigma(z)] \\\\\n",
    "    & = -(\\hat y^n - \\sigma(z))\n",
    "    \\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    '''Class of Data'''\n",
    "    def __init__(self, features, label=None):\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "            \n",
    "    def __str__(self):\n",
    "        return f'Data({self.features}, {self.label})'\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "def f(data, w, b):\n",
    "    sigmoid = lambda x: 1 / (1 + exp(-x))\n",
    "    return sigmoid(sum(x_i * w_i for x_i, w_i in zip(data.features, w)) + b)\n",
    "\n",
    "def loss(training_data_set, w , b):\n",
    "    error = 0\n",
    "    for d in training_data_set:\n",
    "        error += -(d.label * log(f(d, w, b)) + (1-d.label) * log(1 - f(d, w, b)))\n",
    "    return error\n",
    "\n",
    "def cal_gradient(training_data_set, w, b):\n",
    "    grad = dict(w=[0]*len(w), b=0)\n",
    "    \n",
    "    for i, w_i in enumerate(w):\n",
    "        grad['w'][i] = sum(-(d.label - f(d, w, b)) * d.features[i] for d in training_data_set) / len(training_data_set)\n",
    "    grad['b'] = sum(-(d.label - f(d, w, b)) for d in training_data_set) / len(training_data_set)\n",
    "    return grad\n",
    "\n",
    "def gradient_descent(env, grad):\n",
    "    for i, wi in enumerate(grad['w']):\n",
    "        env.w[i] -= env.l * wi\n",
    "    env.b -= env.l * grad['b']\n",
    "    \n",
    "def update_learning_rate(env):\n",
    "    env.l *= env.al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化環境並準備資料\n",
    "\n",
    "由於只是個很小的例子，因此並沒有另外準備資料，而是在 `read_data()` 中直接設定參數並參生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, w, b, l, al):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.l = l\n",
    "        self.al = al\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'w={self.w}, b={self.b}, learning rate={self.l}, adapt rate ={self.al}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def initial():\n",
    "    '''Return env which contain w, b, l, and al(adaptive_l)'''\n",
    "    w = [random.random(), random.random()]\n",
    "    b = random.random()\n",
    "    l = 0.3\n",
    "    al = 0.9\n",
    "    return Env(w, b , l, al)\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    following tuple all have 4 element, respond to number/label of data that have feature (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "    '''\n",
    "    label = (0, 0, 0, 1)\n",
    "    training_distribution = (4, 4, 4, 1)\n",
    "    noise_distribution = (0, 0, 0, 0)\n",
    "    validation_distribution = (0, 0, 1, 1)\n",
    "    \n",
    "    DataSets = namedtuple('DataSets', 'training, validation')\n",
    "    data_type = [(f1, f2) for f1 in range(2) for f2 in range(2)]\n",
    "    \n",
    "    data_set = DataSets([], [])\n",
    "    for num, l, d in zip(training_distribution, label, data_type):\n",
    "        data_set.training.extend(Data(d, l) for _ in range(num))\n",
    "    \n",
    "    for num, l, d in zip(noise_distribution, label, data_type):\n",
    "        data_set.training.extend(Data(d, int(not l )) for _ in range(num))\n",
    "        \n",
    "    for num, l, d in zip(validation_distribution, label, data_type):\n",
    "        data_set.validation.extend(Data(d, l) for _ in range(num))\n",
    "    \n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 監控環境變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_env(env, header='Initial'):\n",
    "    print(f'{header}: {env}')\n",
    "\n",
    "\n",
    "def print_valid(data_set, w, b, header='Initial'):\n",
    "    print(f'{header}: ', end='')\n",
    "    for d in data_set:\n",
    "        print('f({}) = {}'.format(d, f(d, w, b)), end= ', ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: w=[0.7171448575830212, 0.20076584819593768], b=0.14139380290539472, learning rate=0.3, adapt rate =0.9\n",
      "(Val) Round 50: f(Data((1, 0), 0)) = 0.14313693019764173, f(Data((1, 1), 1)) = 0.15635221164636176, \n",
      "(Val) Round 100: f(Data((1, 0), 0)) = 0.12279608554911947, f(Data((1, 1), 1)) = 0.1957238980804802, \n",
      "(Env) Round 100: w=[0.7018376673253899, 0.5529767026687703], b=-2.6680520077283343, learning rate=0.27, adapt rate =0.9\n",
      "gd_w=[-0.02412048198084291, -0.028831021814464855], gd_b = 0.029130644508182164, loss= 2.879000241542745\n",
      "(Val) Round 150: f(Data((1, 0), 0)) = 0.11711833188052842, f(Data((1, 1), 1)) = 0.2487264748171185, \n",
      "(Val) Round 200: f(Data((1, 0), 0)) = 0.11188854010559092, f(Data((1, 1), 1)) = 0.3001894804463901, \n",
      "(Env) Round 200: w=[1.290096267434316, 1.2251983195847997], b=-3.361690322300477, learning rate=0.24300000000000002, adapt rate =0.9\n",
      "gd_w=[-0.01944727930164355, -0.021399091416470322], gd_b = 0.023436482617606322, loss= 2.2607390011110438\n",
      "(Val) Round 250: f(Data((1, 0), 0)) = 0.10701200708246626, f(Data((1, 1), 1)) = 0.3425529705937595, \n",
      "(Val) Round 300: f(Data((1, 0), 0)) = 0.10214914969095266, f(Data((1, 1), 1)) = 0.3810297303984813, \n",
      "(Env) Round 300: w=[1.7204820443737343, 1.6883901292465988], b=-3.894052010804289, learning rate=0.21870000000000003, adapt rate =0.9\n",
      "gd_w=[-0.01620942257532359, -0.017110334938758073], gd_b = 0.020520227168544094, loss= 1.8945955494937639\n",
      "(Val) Round 350: f(Data((1, 0), 0)) = 0.09789920353065813, f(Data((1, 1), 1)) = 0.4125915608258267, \n",
      "(Val) Round 400: f(Data((1, 0), 0)) = 0.09382595429831174, f(Data((1, 1), 1)) = 0.44154131875345765, \n",
      "(Env) Round 400: w=[2.050659564209498, 2.0328808437589982], b=-4.318449439211959, learning rate=0.19683000000000003, adapt rate =0.9\n",
      "gd_w=[-0.014107066467958614, -0.014571799755884416], gd_b = 0.018383021296428336, loss= 1.6519861834299663\n",
      "(Val) Round 450: f(Data((1, 0), 0)) = 0.09033216564193919, f(Data((1, 1), 1)) = 0.465581613727277, \n",
      "(Val) Round 500: f(Data((1, 0), 0)) = 0.08700827578054339, f(Data((1, 1), 1)) = 0.4879001764867999, \n",
      "(Env) Round 500: w=[2.313137908357814, 2.302314832841401], b=-4.663861486485062, learning rate=0.17714700000000003, adapt rate =0.9\n",
      "gd_w=[-0.012633696260965879, -0.01289849964341997], gd_b = 0.01677706256268959, loss= 1.4796670938584304\n",
      "(Val) Round 550: f(Data((1, 0), 0)) = 0.0841620586813043, f(Data((1, 1), 1)) = 0.5066457171367917, \n",
      "(Val) Round 600: f(Data((1, 0), 0)) = 0.08145054467121118, f(Data((1, 1), 1)) = 0.5242310681074113, \n",
      "(Env) Round 600: w=[2.5269135201266457, 2.519799982200596], b=-4.949713244702313, learning rate=0.15943230000000003, adapt rate =0.9\n",
      "gd_w=[-0.011545843846756495, -0.011709875742633529], gd_b = 0.015538840598267677, loss= 1.3514307569151258\n",
      "(Val) Round 650: f(Data((1, 0), 0)) = 0.0791218104244087, f(Data((1, 1), 1)) = 0.539143509857527, \n",
      "(Val) Round 700: f(Data((1, 0), 0)) = 0.07689483712872393, f(Data((1, 1), 1)) = 0.5532557750630456, \n",
      "(Env) Round 700: w=[2.7041148655282656, 2.699138617348816], b=-5.189419292689507, learning rate=0.14348907000000002, adapt rate =0.9\n",
      "gd_w=[-0.010712721758972293, -0.01082161165711448], gd_b = 0.014561635549536007, loss= 1.2527437385392854\n",
      "(Val) Round 750: f(Data((1, 0), 0)) = 0.07497417585855018, f(Data((1, 1), 1)) = 0.5653195102952988, \n",
      "(Val) Round 800: f(Data((1, 0), 0)) = 0.07312943659168429, f(Data((1, 1), 1)) = 0.5768198914464473, \n",
      "(Env) Round 800: w=[2.8529787809866853, 2.8493151454906767], b=-5.3925617322038235, learning rate=0.12914016300000003, adapt rate =0.9\n",
      "gd_w=[-0.010057107425450407, -0.010133659516978576], gd_b = 0.013775331195825535, loss= 1.1748460903706357\n",
      "(Val) Round 850: f(Data((1, 0), 0)) = 0.07153148848401944, f(Data((1, 1), 1)) = 0.586717457874198, \n",
      "(Val) Round 900: f(Data((1, 0), 0)) = 0.06999016115926361, f(Data((1, 1), 1)) = 0.5962113140626364, \n",
      "(Env) Round 900: w=[2.9793576249571294, 2.9765439596995398], b=-5.566198113144176, learning rate=0.11622614670000003, adapt rate =0.9\n",
      "gd_w=[-0.009530227899988962, -0.00958668056114157], gd_b = 0.013132491992154374, loss= 1.1121251364653937\n",
      "(Val) Round 950: f(Data((1, 0), 0)) = 0.06864952187009382, f(Data((1, 1), 1)) = 0.6044287810645038, \n",
      "(Val) Round 1000: f(Data((1, 0), 0)) = 0.06735128481297996, f(Data((1, 1), 1)) = 0.6123525159407673, \n",
      "(Env) Round 1000: w=[3.0875567891837803, 3.085318382279896], b=-5.715663428746111, learning rate=0.10460353203000003, adapt rate =0.9\n",
      "gd_w=[-0.009099676656018211, -0.009143010491831395], gd_b = 0.012600001799590891, loss= 1.0608122323615898\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = initial()\n",
    "    data_set = read_data()\n",
    "    print_env(env)\n",
    "    \n",
    "    for times in range(1000):\n",
    "        random.shuffle(data_set.training)\n",
    "        grad = cal_gradient(data_set.training, env.w, env.b)\n",
    "        gradient_descent(env, grad)\n",
    "        if not (times+1) % 50:\n",
    "            print_valid(data_set.validation, env.w, env.b, header='(Val) Round {}'.format(times+1))\n",
    "\n",
    "        if not (times+1) % 100:\n",
    "            update_learning_rate(env)\n",
    "        \n",
    "            print_env(env, header='(Env) Round {}'.format(times+1))\n",
    "            print(f'gd_w={grad[\"w\"]}, gd_b = {grad[\"b\"]}, loss=', loss(data_set.training, env.w, env.b))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
