{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 測試 Discriminative Model\n",
    "李宏毅 ML Lecture 5 中，關於 generative model 可能表現比較差的原因有給予一例試著說明\n",
    "但並沒有給予相對應的 discriminative model 測試\n",
    "本 notebook 以此為例實做並驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "        def __init__(self, features, label=None):\n",
    "            self.features = features\n",
    "            self.label = label\n",
    "            \n",
    "        def __str__(self):\n",
    "            return f'[{self.features}, {self.label}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 對 B 做微分\n",
    "$\\begin{align}\n",
    "L(f) & = \\frac{1}{n} \\sum_n -[\\hat y^n(1-\\sigma(z)) - (1- \\hat y^n) \\sigma(z)] \\\\\n",
    "    & = \\frac{1}{n} \\sum_n -[\\hat y^n - \\hat y^n \\sigma(z) - \\sigma(z) + \\hat y^n  \\sigma(z)] \\\\\n",
    "    & = -(\\hat y^n - \\sigma(z))\n",
    "    \\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(data, w, b):\n",
    "    sigmoid = lambda x: 1 / (1 + exp(-x))\n",
    "    return sigmoid(sum(x_i * w_i for x_i, w_i in zip(data.features, w)) + b)\n",
    "\n",
    "def loss(training_data_set, w , b):\n",
    "    error = 0\n",
    "    for d in training_data_set:\n",
    "        error += -(d.label * log(f(d, w, b)) + (1 - d.label) * log(1 - f(d, w, b)))\n",
    "    return error\n",
    "\n",
    "def cal_gd_w(training_data_set, w, b):\n",
    "    gd_w = [0] * len(w)\n",
    "    for i, w_i in enumerate(w):\n",
    "        gd_w = sum(-(d.label - f(d, w, b) * d.features[i]) for d in training_data_set) / len(training_data_set)\n",
    "    return gd_w\n",
    "\n",
    "def cal_gd_b(training_data_set, w, b):\n",
    "    gd_b = sum(-(d.label - f(d, w, b)) for d in training_data_set) / len(training_data_set)\n",
    "    return gd_b\n",
    "\n",
    "def initial(sample_distribution):\n",
    "    '''\n",
    "    sample_distribution:\n",
    "        list-like item contain 4 interger, as number of data with feature (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "    '''\n",
    "    training_data_set = []\n",
    "    for sample_number in sample_distribution:\n",
    "        for first_feature in range(2):\n",
    "            for second_feature in range(2):\n",
    "                label = 1 if first_feature and second_feature else 0\n",
    "                training_data_set += [Data((first_feature, second_feature), label) for i in range(sample_number)]\n",
    "\n",
    "    w = [random.random(), random.random()]\n",
    "    b = random.random()\n",
    "    l = 0.3\n",
    "\n",
    "    return training_data_set, w, b, l\n",
    "\n",
    "\n",
    "def print_env(data_set, w, b, l, header='Initial'):\n",
    "    print(f'{header}: w = {w}, b = {b}, learning rate = {l}')\n",
    "    print(f'loss =', loss(data_set, w, b))\n",
    "\n",
    "\n",
    "def main():\n",
    "    training_data_set, w, b, l = initial(sample_distribution=[4, 4, 4, 1])\n",
    "#     x = [(1, 1)] * 1 + [(1, 0)] * 4 + [(0, 1)] * 4 + [(0, 0)] * 4\n",
    "#     random.shuffle(x)\n",
    "#     y = [1]*12 + [0]*12\n",
    "    print_env(training_data_set, w, b, l)\n",
    "    for i in range(1000):\n",
    "        random.shuffle(traing_data_set)\n",
    "        gd_w = cal_gd_w(training_data_set, w, b)\n",
    "        move = gd(x, y, w, b)\n",
    "        \n",
    "        if not (i+1)% 100:\n",
    "            print_env(training_data_set, w, b, l, 'Round {}'.format(i+1))\n",
    "            print(f', move = {move}')\n",
    "            print('f([1, 1]) =', f((1, 1), w, b), ', f([1, 0]) =', f((1, 0), w, b))\n",
    "            l *= 0.5\n",
    "\n",
    "        b = b - l * gd_b(x, y, w, b)\n",
    "        for i, move_i in enumerate(move):\n",
    "            w[i] = w[i] - l * move_i\n",
    "\n",
    "    print(f'x={x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(data, w, b):\n",
    "    sigmoid = lambda x: 1 / (1 + exp(-x))\n",
    "    return sigmoid(sum(x_i * w_i for x_i, w_i in zip(data.features, w)) + b)\n",
    "\n",
    "def loss(training_data_set, w , b):\n",
    "    error = 0\n",
    "    for d in training_data_set:\n",
    "        error += -(d.label * log(f(d, w, b)) + (1-d.label) * log(1 - f(d, w, b)))\n",
    "    return error\n",
    "\n",
    "def cal_gd_w(training_data_set, w, b):\n",
    "    gd_w = [0] * len(w)\n",
    "    for i, w_i in enumerate(w):\n",
    "        gd_w[i] = sum(-(d.label - f(d, w, b) * d.features[i]) for d in training_data_set) / len(training_data_set)\n",
    "    return gd_w\n",
    "\n",
    "def cal_gd_b(training_data_set, w, b):\n",
    "    gd_b = sum(-(d.label - f(d, w, b)) for d in training_data_set) / len(training_data_set)\n",
    "    return gd_b\n",
    "\n",
    "def initial(sample_distribution):\n",
    "    '''\n",
    "    sample_distribution:\n",
    "        list-like item contain 4 interger, as number of data with feature (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "    '''\n",
    "    training_data_set = []\n",
    "    for sample_number in sample_distribution:\n",
    "        for first_feature in range(2):\n",
    "            for second_feature in range(2):\n",
    "                label = 1 if first_feature and second_feature else 0\n",
    "                training_data_set += [Data((first_feature, second_feature), label) for i in range(sample_number)]\n",
    "\n",
    "    w = [random.random(), random.random()]\n",
    "    b = random.random()\n",
    "    l = 0.3\n",
    "\n",
    "    return training_data_set, w, b, l\n",
    "\n",
    "\n",
    "def print_env(data_set, w, b, l, header='Initial'):\n",
    "    print(f'{header}: w = {w}, b = {b}, learning rate = {l}')\n",
    "    print(f'loss =', loss(data_set, w, b))\n",
    "\n",
    "def print_test(data_set, w, b):\n",
    "    for d in data_set:\n",
    "        print('f({}) = {}'.format(d, f(d, w, b)), end= ', ')\n",
    "\n",
    "def main():\n",
    "    training_data_set, w, b, l = initial(sample_distribution=[4, 4, 4, 1])\n",
    "    testing_data_set = [Data((1, 1)), Data((1, 0))]\n",
    "#     x = [(1, 1)] * 1 + [(1, 0)] * 4 + [(0, 1)] * 4 + [(0, 0)] * 4\n",
    "#     random.shuffle(x)\n",
    "#     y = [1]*12 + [0]*12\n",
    "    print_env(training_data_set, w, b, l)\n",
    "    for times in range(1000):\n",
    "        random.shuffle(training_data_set)\n",
    "        gd_w = cal_gd_w(training_data_set, w, b)\n",
    "        gd_b = cal_gd_b(training_data_set, w, b)\n",
    "        \n",
    "       # Updating weight\n",
    "        b = b - l * gd_b\n",
    "        for i, gd_w in enumerate(gd_w):\n",
    "            w[i] -= l * gd_w\n",
    "            \n",
    "        if not (times+1) % 100:\n",
    "            l *= 0.5\n",
    "            \n",
    "            # Export Env\n",
    "            print_env(training_data_set, w, b, l, 'Round {}'.format(times+1))\n",
    "            print('gd_w = {}, gd_b = {}', gd_w, gd_b)\n",
    "    print_test(testing_data_set, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: w = [0.23993368387120584, 0.4650123013357019], b = 0.835341271080032, learning rate = 0.3\n",
      "loss = 55.899103251262176\n",
      "Round 100: w = [1.3698027987492127, 1.4139369978817713], b = -2.4182306778791802, learning rate = 0.15\n",
      "loss = 15.923798308123251\n",
      "gd_w = {}, gd_b = {} -0.03558680683752785 0.05020730502818559\n",
      "Round 200: w = [1.8604475302373116, 1.8824263715512168], b = -3.0866617053617613, learning rate = 0.075\n",
      "loss = 12.76795845628348\n",
      "gd_w = {}, gd_b = {} -0.02777531886930533 0.03998777880877727\n",
      "Round 300: w = [2.0644067821576755, 2.080307696161941], b = -3.3724122061425796, learning rate = 0.0375\n",
      "loss = 11.641570660794692\n",
      "gd_w = {}, gd_b = {} -0.025144390582333233 0.03640822550820725\n",
      "Round 400: w = [2.1588231612226907, 2.172426137559618], b = -3.5059541745844647, learning rate = 0.01875\n",
      "loss = 11.153024127246711\n",
      "gd_w = {}, gd_b = {} -0.024026622626982796 0.034869196150100444\n",
      "Round 500: w = [2.204377201164385, 2.216976331303533], b = -3.570642872686294, learning rate = 0.009375\n",
      "loss = 10.92439189680758\n",
      "gd_w = {}, gd_b = {} -0.02350806523356843 0.03415177433341135\n",
      "Round 600: w = [2.226765672391961, 2.238895117811549], b = -3.6024937833506723, learning rate = 0.0046875\n",
      "loss = 10.81367650132637\n",
      "gd_w = {}, gd_b = {} -0.023257958197464172 0.03380500057606113\n",
      "Round 700: w = [2.2378656553711753, 2.249767862951656], b = -3.6182990448046337, learning rate = 0.00234375\n",
      "loss = 10.759183738415219\n",
      "gd_w = {}, gd_b = {} -0.02313509515568805 0.03363447444890861\n",
      "Round 800: w = [2.24339243057951, 2.2551828663436395], b = -3.626172009702713, learning rate = 0.001171875\n",
      "loss = 10.732149380139687\n",
      "gd_w = {}, gd_b = {} -0.02307419908360799 0.03354991182606521\n",
      "Round 900: w = [2.2461500566529846, 2.2578850617018853], b = -3.6301011227376767, learning rate = 0.0005859375\n",
      "loss = 10.71868469210094\n",
      "gd_w = {}, gd_b = {} -0.023043883426120584 0.033507803822895206\n",
      "Round 1000: w = [2.2475274345743195, 2.2592348372513977], b = -3.6320638427337575, learning rate = 0.00029296875\n",
      "loss = 10.711965407228474\n",
      "gd_w = {}, gd_b = {} -0.023028758508850576 0.03348679292566454\n",
      "f([(1, 1), None]) = 0.7057224019912902, f([(1, 0), None]) = 0.20028142081856556, "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
