{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 測試 Discriminative Model\n",
    "李宏毅 ML Lecture 5 中，關於 generative model 可能表現比較差的原因有給予一例試著說明\n",
    "但並沒有給予相對應的 discriminative model 測試\n",
    "本 notebook 以此為例實做並驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "        def __init__(self, features, label=None):\n",
    "            self.features = features\n",
    "            self.label = label\n",
    "            \n",
    "        def __str__(self):\n",
    "            return f'[{self.features}, {self.label}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 對 B 做微分\n",
    "$\\begin{align}\n",
    "L(f) & = \\frac{1}{n} \\sum_n -[\\hat y^n(1-\\sigma(z)) - (1- \\hat y^n) \\sigma(z)] \\\\\n",
    "    & = \\frac{1}{n} \\sum_n -[\\hat y^n - \\hat y^n \\sigma(z) - \\sigma(z) + \\hat y^n  \\sigma(z)] \\\\\n",
    "    & = -(\\hat y^n - \\sigma(z))\n",
    "    \\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(data, w, b):\n",
    "    sigmoid = lambda x: 1 / (1 + exp(-x))\n",
    "    return sigmoid(sum(x_i * w_i for x_i, w_i in zip(data.features, w)) + b)\n",
    "\n",
    "def loss(training_data_set, w , b):\n",
    "    error = 0\n",
    "    for d in training_data_set:\n",
    "        error += -(d.label * log(f(d, w, b)) + (1-d.label) * log(1 - f(d, w, b)))\n",
    "    return error\n",
    "\n",
    "def cal_gd_w(training_data_set, w, b):\n",
    "    gd_w = [0] * len(w)\n",
    "    for i, w_i in enumerate(w):\n",
    "        gd_w[i] = sum(-(d.label - f(d, w, b) * d.features[i]) for d in training_data_set) / len(training_data_set)\n",
    "    return gd_w\n",
    "\n",
    "def cal_gd_b(training_data_set, w, b):\n",
    "    gd_b = sum(-(d.label - f(d, w, b)) for d in training_data_set) / len(training_data_set)\n",
    "    return gd_b\n",
    "\n",
    "def initial(sample_distribution):\n",
    "    '''\n",
    "    sample_distribution:\n",
    "        list-like item contain 4 interger, as number of data with feature (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "    '''\n",
    "    training_data_set = []\n",
    "    for sample_number in sample_distribution:\n",
    "        for first_feature in range(2):\n",
    "            for second_feature in range(2):\n",
    "                label = 1 if first_feature and second_feature else 0\n",
    "                training_data_set += [Data((first_feature, second_feature), label) for i in range(sample_number)]\n",
    "\n",
    "    w = [random.random(), random.random()]\n",
    "    b = random.random()\n",
    "    l = 0.3\n",
    "\n",
    "    return training_data_set, w, b, l\n",
    "\n",
    "\n",
    "def print_env(data_set, w, b, l, header='Initial'):\n",
    "    print(f'{header}: w = {w}, b = {b}, learning rate = {l}')\n",
    "    print(f'loss =', loss(data_set, w, b))\n",
    "\n",
    "def print_test(data_set, w, b, header='Initial'):\n",
    "    print(f'{header}: ', end='')\n",
    "    for d in data_set:\n",
    "        print('f({}) = {}'.format(d, f(d, w, b)), end= ', ')\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    training_data_set, w, b, l = initial(sample_distribution=[4, 4, 4, 1])\n",
    "    testing_data_set = [Data((1, 1)), Data((1, 0))]\n",
    "    \n",
    "    print_env(training_data_set, w, b, l)\n",
    "    for times in range(1000):\n",
    "        random.shuffle(training_data_set)\n",
    "        gd_w = cal_gd_w(training_data_set, w, b)\n",
    "        gd_b = cal_gd_b(training_data_set, w, b)\n",
    "        \n",
    "       # Updating weight\n",
    "        b = b - l * gd_b\n",
    "        for i, gd_w in enumerate(gd_w):\n",
    "            w[i] -= l * gd_w\n",
    "            \n",
    "        if not (times+1) % 100:\n",
    "            l *= 0.9\n",
    "            \n",
    "            # Export Env\n",
    "            print_env(training_data_set, w, b, l, 'Round {}'.format(times+1))\n",
    "            print('gd_w = {}, gd_b = {}', gd_w, gd_b)\n",
    "        \n",
    "        if not (times+1) % 50:\n",
    "            print_test(testing_data_set, w, b, header='(Val) Round {}'.format(times+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: w = [0.1618279687505364, 0.0269260502685863], b = 0.1834607919821709, learning rate = 0.3\n",
      "loss = 38.97168639731798\n",
      "(Val) Round 50: f([(1, 1), None]) = 0.4893671905071753, f([(1, 0), None]) = 0.3113705884527738, \n",
      "Round 100: w = [1.4354607154356815, 1.408960517666553], b = -2.459797595881627, learning rate = 0.27\n",
      "loss = 15.701753306140484\n",
      "gd_w = {}, gd_b = {} -0.03671210418223761 0.04949140995495206\n",
      "(Val) Round 100: f([(1, 1), None]) = 0.5949877822150302, f([(1, 0), None]) = 0.26418349010446596, \n",
      "(Val) Round 150: f([(1, 1), None]) = 0.6559854072827758, f([(1, 0), None]) = 0.23138584506339033, \n",
      "Round 200: w = [2.2085341382040737, 2.200600448018253], b = -3.5618435730417404, learning rate = 0.24300000000000002\n",
      "loss = 10.955105454369953\n",
      "gd_w = {}, gd_b = {} -0.02406909517205754 0.0343434473007312\n",
      "(Val) Round 200: f([(1, 1), None]) = 0.6999985620846412, f([(1, 0), None]) = 0.20532984634880797, \n",
      "(Val) Round 250: f([(1, 1), None]) = 0.7306848555786265, f([(1, 0), None]) = 0.18616835356716932, \n",
      "Round 300: w = [2.716947599262546, 2.713802284242284], b = -4.301931985303216, learning rate = 0.21870000000000003\n",
      "loss = 8.658139982353667\n",
      "gd_w = {}, gd_b = {} -0.018723506682740785 0.027235419135777896\n",
      "(Val) Round 300: f([(1, 1), None]) = 0.75562068042531, f([(1, 0), None]) = 0.17009072928849056, \n",
      "(Val) Round 350: f([(1, 1), None]) = 0.77443751525895, f([(1, 0), None]) = 0.15769902698076899, \n",
      "Round 400: w = [3.0892753202447882, 3.0877588493478987], b = -4.848442950275137, learning rate = 0.19683000000000003\n",
      "loss = 7.283710258733716\n",
      "gd_w = {}, gd_b = {} -0.015697015785414638 0.02303189147218301\n",
      "(Val) Round 400: f([(1, 1), None]) = 0.7906075104642569, f([(1, 0), None]) = 0.14689461891232583, \n",
      "(Val) Round 450: f([(1, 1), None]) = 0.8033405766448152, f([(1, 0), None]) = 0.13829627555818935, \n",
      "Round 500: w = [3.3770829190514338, 3.3762411268977326], b = -5.2727816988501415, learning rate = 0.17714700000000003\n",
      "loss = 6.3686009558367935\n",
      "gd_w = {}, gd_b = {} -0.01372836558799559 0.020238024338620843\n",
      "(Val) Round 500: f([(1, 1), None]) = 0.8146544850563439, f([(1, 0), None]) = 0.1305960625742871, \n",
      "(Val) Round 550: f([(1, 1), None]) = 0.8238110231196112, f([(1, 0), None]) = 0.12432620072846044, \n",
      "Round 600: w = [3.6071014689762873, 3.6065824751961975], b = -5.612856471013681, learning rate = 0.15943230000000003\n",
      "loss = 5.717812065191893\n",
      "gd_w = {}, gd_b = {} -0.012342478330987837 0.018247203554101598\n",
      "(Val) Round 600: f([(1, 1), None]) = 0.8321340041489839, f([(1, 0), None]) = 0.11860000666998982, \n",
      "(Val) Round 650: f([(1, 1), None]) = 0.8390017510578857, f([(1, 0), None]) = 0.11385689778130002, \n",
      "Round 700: w = [3.79519500265301, 3.7948482145569664], b = -5.8914660693830445, learning rate = 0.14348907000000002\n",
      "loss = 5.233554707046538\n",
      "gd_w = {}, gd_b = {} -0.011315697700332344 0.016761110831353798\n",
      "(Val) Round 700: f([(1, 1), None]) = 0.8453488108683458, f([(1, 0), None]) = 0.10945978241452801, \n",
      "(Val) Round 750: f([(1, 1), None]) = 0.850662667184134, f([(1, 0), None]) = 0.10576895100815535, \n",
      "Round 800: w = [3.951607420695732, 3.951360591796725], b = -6.123457869061029, learning rate = 0.12914016300000003\n",
      "loss = 4.861053792152473\n",
      "gd_w = {}, gd_b = {} -0.010527160666699564 0.015614105222515107\n",
      "(Val) Round 800: f([(1, 1), None]) = 0.8556363680079898, f([(1, 0), None]) = 0.10230696269885896, \n",
      "(Val) Round 850: f([(1, 1), None]) = 0.8598476341393738, f([(1, 0), None]) = 0.09937032211934826, \n",
      "Round 900: w = [4.083352134360081, 4.083167342257819], b = -6.31905653378073, learning rate = 0.11622614670000003\n",
      "loss = 4.567155451862018\n",
      "gd_w = {}, gd_b = {} -0.009905208484786756 0.014706217296561611\n",
      "(Val) Round 900: f([(1, 1), None]) = 0.863828948144528, f([(1, 0), None]) = 0.09658972747234196, \n",
      "(Val) Round 950: f([(1, 1), None]) = 0.8672303926363265, f([(1, 0), None]) = 0.09421093054420578, \n",
      "Round 1000: w = [4.195443798204941, 4.195299642688937], b = -6.485603754398876, learning rate = 0.10460353203000003\n",
      "loss = 4.330594279773534\n",
      "gd_w = {}, gd_b = {} -0.009404427762302424 0.01397331123802546\n",
      "(Val) Round 1000: f([(1, 1), None]) = 0.8704721316035299, f([(1, 0), None]) = 0.0919411946347752, \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
